{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_Hogeschool-Utrecht_Reinforcement-Learning-project_Deep-Q-Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basanaz/hu-reinforcement-learning-gym/blob/master/2_Hogeschool_Utrecht_Reinforcement_Learning_project_Deep_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eU2QJ--T4dd"
      },
      "source": [
        "# Reinforcement Learning project - Deep Q-Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qdkwbpRUhzO"
      },
      "source": [
        "## Aim\n",
        "Time to up our [OpenAI Gym](https://gym.openai.com/) game with some [Atari Space Invaders](https://gym.openai.com/envs/SpaceInvaders-v0/) (or another [supported Atari game](https://gym.openai.com/envs/#atari)!). Q-Learning won't cut it this time. Let's take out the DQNs and beat this game!\n",
        "\n",
        "## Runtime and environment\n",
        "This [Jupyter Notebook](https://jupyterlab.readthedocs.io/en/latest/) was made to run on Google Colab. For this training, we recommend using the Google Colab environment.\n",
        "\n",
        "Please read the [instructions on Google Colab](https://medium.com/swlh/the-best-place-to-get-started-with-ai-google-colab-tutorial-for-beginners-715e64bb603b) to get started quickly. It behaves similar to Jupyter Notebook, Jupyter Hub and Jupyter Lab, so if you have any experience with those, you're good to go!\n",
        "\n",
        "Some notes on Google Colab:\n",
        "- **Processes in Google Colab won't run forever**. These may be terminated at any time when the platform is crowded, and *will definitely* terminate after 12 hours. To maintain persistency, you can attach the session to **Google Drive** and have your models persist themselves to the Google Drive periodically.\n",
        "- You can enable GPU or TPU support! You can find this option under *Runtime* -> *Change runtime type*.\n",
        "- After installing dependencies, you need to restart the runtime in order to actually use them.\n",
        "\n",
        "If you want to run the code on your own platform or system, you need to keep a few things in mind:\n",
        "- The dependencies you need to install may differ from the ones we installed here. The installed dependencies are suitable for Google Colab, Ubuntu, and Debian.\n",
        "- Since Google Colab isn't attached to a monitor, we render the output to a video file. On your own machine the built-in render method from OpenAI's Gym may suffice.\n",
        "- The default paths use Google Drive! Change these.\n",
        "\n",
        "## Info Support\n",
        "This assignment was developed by Info Support. Looking for a graduation project or job? Check out their website: https://carriere.infosupport.com/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7mfjQyuT_zV"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_LwZ18PXsaL"
      },
      "source": [
        "Some dependencies need to be installed for the code to work. Furthermore, we will define some methods which allow us to show the OpenAI Gym renderings in this (headless) Google Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBdwK87YUI9Y"
      },
      "source": [
        "# Install dependencies\n",
        "\"\"\"Note: if you are running this code on your own machine, you probably don't need all of these.\n",
        "   Start with 'pip install gym' and install more packages if you run into errors.\"\"\"\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg cmake > /dev/null 2>&1\n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install atari-py > /dev/null 2>&1\n",
        "!pip install torchagent > /dev/null 2>&1\n",
        "\n",
        "# New requirement!\n",
        "!pip install colabgymrender"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fQVLnDPxNOz"
      },
      "source": [
        "# Install the Atari ROMs\n",
        "!wget -O Roms.rar http://www.atarimania.com/roms/Roms.rar\n",
        "!unrar e Roms.rar\n",
        "!python -m atari_py.import_roms ./ > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7IG_wxod9mW"
      },
      "source": [
        "# Imports for helper functions\n",
        "import base64\n",
        "import copy\n",
        "import io\n",
        "import math\n",
        "from pathlib import Path\n",
        "\n",
        "import gym\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from colabgymrender.recorder import Recorder        # New import!!\n",
        "from google.colab import drive\n",
        "from gym.wrappers import Monitor\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFDD54_Afgs4"
      },
      "source": [
        "# Mount your Google Drive. By doing so, you can store any output, models, videos, and images persistently.\n",
        "\n",
        "# Note: in this new version of the notebook, we no longer render video's to Google Drive, but to a temporary storage within the container!\n",
        "# Please copy files from /tmp/HU_RL/ to /content/gdrive/My Drive/<my_path> that you want to keep, or just download them through the viewer!\n",
        "\n",
        "enable_gdrive = False   # Switch to indicate whether Google Drive should be mounted.\n",
        "\n",
        "if enable_gdrive:\n",
        "    drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJeiCQFRgnSj"
      },
      "source": [
        "# Create a directory to store the data for this lab. Feel free to change this.\n",
        "if enable_gdrive:\n",
        "    drive_path = Path('/content/gdrive/My Drive/Colab Notebooks/HU_RL/part1')\n",
        "    drive_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "tmp_path_on_container = Path('/tmp/HU_RL/')\n",
        "video_path = tmp_path_on_container / 'video'\n",
        "video_path.mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU2ZsnBEelqW"
      },
      "source": [
        "# Define helper functions to visually show what the models are doing.\n",
        "%matplotlib inline\n",
        "\n",
        "gym.logger.set_level(gym.logger.ERROR)\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "# def show_video():\n",
        "#     # Display the stored video file\n",
        "#     # Credits: https://star-ai.github.io/Rendering-OpenAi-Gym-in-Colaboratory/\n",
        "#     mp4list = list(data_path.glob('video/*.mp4'))\n",
        "#     if len(mp4list) > 0:\n",
        "#         mp4 = mp4list[-1]\n",
        "#         video = io.open(mp4, 'r+b').read()\n",
        "#         encoded = base64.b64encode(video)\n",
        "#         ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "#                 loop controls style=\"height: 400px;\">\n",
        "#                 <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "#             </video>'''.format(encoded.decode('ascii'))))\n",
        "#     else: \n",
        "#         print('Could not find video')\n",
        "\n",
        "# def record_episode(idx):\n",
        "#     # This determines which episodes to record.\n",
        "#     # Since the video rendering in the OpenAI Gym is a bit buggy, we simply override it and decide\n",
        "#     # whether or not to render inside of our training loop.\n",
        "#     return True\n",
        "\n",
        "def video_env(env):\n",
        "    # Wraps the environment to write its output to a video file\n",
        "    # env = Monitor(env, video_path, video_callable=record_episode, force=True)\n",
        "    env = Recorder(env, video_path, 15)     # Render at 15 FPS. Try to keep this low, to prevent hitting quotas.\n",
        "    return env\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShMPjUFXiwli"
      },
      "source": [
        "# Test the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3hAfAGhi4KK"
      },
      "source": [
        "\"\"\"Let's check out the Space Invaders environment\"\"\"\n",
        "\n",
        "# Create the desired environment\n",
        "env = gym.make(\"SpaceInvaders-v0\")\n",
        "\n",
        "# Wrap the environment, to make sure we get to see a fancy video\n",
        "env = video_env(env)\n",
        "\n",
        "# Before you can use a Gym environment, it needs to be reset.\n",
        "state = env.reset()\n",
        "\n",
        "# Perform random actions untill we lose (or win).\n",
        "done = False\n",
        "while not done:\n",
        "    # env.render()  # This line is no longer needed\n",
        "    # The action_space contains all possible actions we can take.\n",
        "    random_action = env.action_space.sample() \n",
        "\n",
        "    state, reward, done, info = env.step(random_action)\n",
        "\n",
        "# Show the results!\n",
        "env.play()  # Render the video\n",
        "# env.close()   # We keep the env open this time\n",
        "# show_video()\n",
        "\n",
        "# So... this works exactly the same as with the previous environments! The only differences are the state space and possible actions."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-O3zaP6q4-E"
      },
      "source": [
        "# Check out the new state:\n",
        "print('State shape:', state.shape)\n",
        "\n",
        "# We have 210 rows, each 160 pixels wide, and each pixel having an RGB value.\n",
        "# The range of each pixel is 0-255\n",
        "\n",
        "print('State:', state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhQlBt2hUAQ2"
      },
      "source": [
        "# Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcxawHEnyRcA"
      },
      "source": [
        "# Imports\n",
        "import random\n",
        "from itertools import count\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchagent.memory import SequentialMemory, Transition\n",
        "from torchagent.policy import DecayingEpsilonGreedyPolicy, EpsilonGreedyPolicy, GreedyPolicy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50ZTsPhDMscQ"
      },
      "source": [
        "# Setup GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S56yXtOAtEvh"
      },
      "source": [
        "# Wrappers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UOQzqlvMs2j"
      },
      "source": [
        "class ResizeWrapper(gym.ObservationWrapper):\n",
        "    # This wrapper resizes and normalizes the input images.\n",
        "    # Feel free to update it! :-)\n",
        "\n",
        "    def __init__(self, env):\n",
        "        super(ResizeWrapper, self).__init__(env)\n",
        "    \n",
        "    def observation(self, observation):\n",
        "        # Resize the image by taking every second pixel\n",
        "        # This reduces the image size from 160x210 to 80x105\n",
        "        # So 105 rows, of 80 pixels each, having 3 RGB values per pixel\n",
        "        img = observation[::2, ::2]\n",
        "\n",
        "        # You can optionally crop the image, to remove irrelevant parts.\n",
        "        # For example, do you really need to top and bottom parts of the screen?\n",
        "        # img = ...\n",
        "\n",
        "        # Convert the image to greyscale\n",
        "        img = img.mean(axis=2)\n",
        "\n",
        "        # Next we normalize the image from -1 to +1\n",
        "        img = (img - 128) / 128 - 1\n",
        "\n",
        "        # Did you crop the image? Don't forget to update these dimensions\n",
        "        return img.reshape(105, 80, 1)\n",
        "\n",
        "class TorchWrapper(gym.ObservationWrapper):\n",
        "    # You don't need to worry about this block of code. It should just work.\n",
        "    def __init__(self, env):\n",
        "        super(TorchWrapper, self).__init__(env)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        output = np.rollaxis(observation, 2, 0)\n",
        "        output = output.reshape(-1, output.shape[0], output.shape[1], output.shape[2]).astype(np.float32)\n",
        "\n",
        "        return torch.from_numpy(output).to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFAEcBfAtUcp"
      },
      "source": [
        "# Environment creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyWicfZAsWI5"
      },
      "source": [
        "def create_environment(name='SpaceInvaders-v0'):\n",
        "    # A shorthand to create a new environment with all of the required wrappers\n",
        "    env = gym.make(name)\n",
        "    env = ResizeWrapper(env)\n",
        "    env = TorchWrapper(env)\n",
        "    env = video_env(env)\n",
        "    return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFJCgw_qw6dU"
      },
      "source": [
        "# DQN Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBywUZk9sX8j"
      },
      "source": [
        "class DQNAgent:\n",
        "    \"\"\"\n",
        "    An implementation of an agent that uses Deep Q-Learning.\n",
        "\n",
        "    It uses the DQN class, defined further down below, to make decisions for its next move.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_actions, model, loss, optimizer,\n",
        "                 memory, policy=None, test_policy=None,\n",
        "                 training=True, enable_dqn=False, batch_size=32,\n",
        "                 gamma=0.9, tau=1e-3, warmup_steps=0, update_steps=1):\n",
        "        \"\"\"\n",
        "        Initializes a new instance of a Deep Q-Learning agent.\n",
        "        Parameters:\n",
        "            num_actions (int): The number of actions that is supported\n",
        "            model (object): The neural network that is used to calculate the q-values for the agent\n",
        "            loss (object): The loss function to use for optimizing the agent\n",
        "            optimizer (object): The optimizer to use for optimizing the agent\n",
        "            memory (object): The experience buffer to use for the agent\n",
        "            policy (object): The policy network to use for the agent\n",
        "            test_policy (object): The target policy network to use for the agent\n",
        "            training (boolean): Flag indicating the agent is training\n",
        "            enable_dqn (boolean): Flag enabling double-q value learning\n",
        "            batch_size (int): The number of samples to use for each cycle of training\n",
        "            gamma (float): The discount factor for rewards the agent receives\n",
        "            tau (float): Factor controlling the speed at which the target model is updated\n",
        "            warmup_steps (int): The number of warmup steps before starting to update the target network\n",
        "        \"\"\"\n",
        "\n",
        "        self.policy = policy if policy is not None else EpsilonGreedyPolicy(num_actions)\n",
        "        self.test_policy = test_policy if test_policy is not None else GreedyPolicy(num_actions)\n",
        "        self.memory = memory\n",
        "        self.training = training\n",
        "        self.batch_size = batch_size\n",
        "        self.model = model\n",
        "        self.target_model = copy.deepcopy(model)\n",
        "        self.enable_dqn = enable_dqn\n",
        "        self.num_actions = num_actions\n",
        "        self.loss = loss\n",
        "        self.gamma = gamma\n",
        "        self.optimizer = optimizer\n",
        "        self.tau = tau\n",
        "        self.step = 0\n",
        "        self.warmup_steps = 0\n",
        "        self.update_steps = 4\n",
        "\n",
        "    def record(self, state, action, next_state, reward, done):\n",
        "        \"\"\"\n",
        "        Records experience for the agent\n",
        "        Parameters:\n",
        "            state (object): The current state\n",
        "            action (object): The action that was performed\n",
        "            next_state (object): The next state\n",
        "            reward (object): The received reward\n",
        "            done (boolean): Flag indicating the episode was completed\n",
        "        \"\"\"\n",
        "        self.memory.append(state, action, next_state, reward, done)\n",
        "\n",
        "    def act(self, observation):\n",
        "        \"\"\"\n",
        "        Allows the agent to perform a step based on the provided observation of the environment.\n",
        "        Parameters:\n",
        "            observation (object): The observation for the agent to base its decision on.\n",
        "        Returns:\n",
        "            int: The index of the selected action\n",
        "        \"\"\"\n",
        "\n",
        "        # First, calculate the q-values for the action space that the agent supports\n",
        "        # Next, use the training or test policy to predict the next action to take\n",
        "        with torch.no_grad():\n",
        "            action_tensor = self.model(observation)\n",
        "\n",
        "        if self.training:\n",
        "            selected_action = self.policy.select_action(action_tensor, step=self.step)\n",
        "        else:\n",
        "            selected_action = self.test_policy.select_action(action_tensor, step=self.step)\n",
        "\n",
        "        return selected_action\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Performs a single training pass using the agent's memory as input.\n",
        "        \"\"\"\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = self.memory.sample(self.batch_size)\n",
        "        batch = Transition(*zip(*batch))\n",
        "\n",
        "        self.step += 1\n",
        "\n",
        "        self.train_model_(batch)\n",
        "        self.update_target_model_()\n",
        "\n",
        "    def train_model_(self, batch):\n",
        "        if self.step < self.warmup_steps or self.step % self.update_steps > 0:\n",
        "            return\n",
        "\n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "        next_state_batch = torch.cat(batch.next_state)\n",
        "        terminated_batch = torch.tensor(\n",
        "            [1. if s == False else 0. for s in batch.done], device=device, dtype=torch.float)\n",
        "\n",
        "        # We're trying to minimize a loss that is defined as follows:\n",
        "        # loss = sum(sqr(reward + gamma * target_Q(next_state) - Q(state)))\n",
        "        #\n",
        "        # Please note that you can use the huber loss also, but it uses the same inputs:\n",
        "        # - Q(next_state, a) --> Q-values for the target state\n",
        "        # - Q(state, a) --> Q-values for the current state\n",
        "        #\n",
        "        # We're using a second network to calculate the Q-values for the target state.\n",
        "        # this stabalizes the training process so it becomes more predictable.\n",
        "        \n",
        "        if not self.enable_dqn:\n",
        "            q_values = self.model(state_batch)\n",
        "            q_values = q_values.gather(1, action_batch)\n",
        "\n",
        "            target_q_values = self.target_model(next_state_batch)\n",
        "            target_q_values = target_q_values.gather(1, action_batch)\n",
        "        else:\n",
        "            # In double-q learning we use the policy network to predict the actions instead of using the actually performed actions.\n",
        "            # We then use these predicted actions to calculate the expected value of these actions.\n",
        "            q_values = self.model(state_batch)\n",
        "            estimated_actions = q_values.max(1)[1]   # meer docstrings toevoegen (TODO)\n",
        "\n",
        "            target_q_values = self.target_model(next_state_batch)\n",
        "            target_q_values = target_q_values.gather(1, estimated_actions)\n",
        "\n",
        "        # Calculate the discounted award for the actions taken by the agent.\n",
        "        # Then reset the reward for the actions that caused the episode to end.\n",
        "        discounted_reward = self.gamma * target_q_values\n",
        "        discounted_reward = terminated_batch * discounted_reward\n",
        "\n",
        "        # Calculate the expected reward using the discounted reward and the actual reward.\n",
        "        targets = discounted_reward + reward_batch\n",
        "        targets = targets.detach()\n",
        "\n",
        "        loss_value = self.loss(q_values, targets.unsqueeze(1))\n",
        "\n",
        "        # Finally, optimize the policy using the choosen optimizer.\n",
        "        self.optimizer.zero_grad()\n",
        "        loss_value.backward()\n",
        "\n",
        "        for param in self.model.parameters():\n",
        "            param.grad.data.clamp(-1, 1)\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_target_model_(self):\n",
        "        if self.step < self.warmup_steps:\n",
        "            return\n",
        "\n",
        "        if self.tau < 1.:\n",
        "            for target_param, param in zip(self.target_model.parameters(), self.model.parameters()):\n",
        "                target_param.data.copy_(\n",
        "                    target_param.data * (1.0 - self.tau) +\n",
        "                    param.data * self.tau   # This line was updated!\n",
        "                )\n",
        "        else:\n",
        "            if self.step % self.tau == 0:\n",
        "                self.target_model.load_state_dict(self.model.state_dict())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3vSmHBNs5s4"
      },
      "source": [
        "# DQN - Network to estimate Q-values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i63Bdw4KMsqS"
      },
      "source": [
        "class DQN(nn.Module):\n",
        "    \"\"\" \n",
        "    This is the network used by the agent to take a decision on its next action.\n",
        "    \"\"\"\n",
        "    def __init__(self, h, w, channels, outputs):\n",
        "        \"\"\"\n",
        "        Initializes a new instance of DQN\n",
        "        Parameters:\n",
        "            h (int): The height of the input screen\n",
        "            w (int): The width of the input screen\n",
        "            channels (int): The number of channels in the input screen\n",
        "            outputs (int): The number of actions to choose from\n",
        "        \"\"\"\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        def conv_output_size(size, kernel_size, stride):\n",
        "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
        "\n",
        "        # Each conv2D layer resizes the input as calculated in the formula above.\n",
        "        # Since we're stacking layers, we need to stack the output size calculation as well.\n",
        "        features_w = conv_output_size(conv_output_size(conv_output_size(w, 8, 4), 4, 2), 3, 1)\n",
        "        features_h = conv_output_size(conv_output_size(conv_output_size(h, 8, 4), 4, 2), 3, 1)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(channels, 32, 8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)\n",
        "        self.fc = nn.Linear(features_w*features_h*64, 512)\n",
        "        self.output = nn.Linear(512, outputs)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.relu4 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Performs the forward pass through the neural network\n",
        "        Parameters:\n",
        "            x (tensor): The input tensor for the policy network\n",
        "        \"\"\"\n",
        "        y = self.relu1(self.conv1(x))\n",
        "        y = self.relu2(self.conv2(y))\n",
        "        y = self.relu3(self.conv3(y))\n",
        "        \n",
        "        # Flatten the output of the convolutional layer.\n",
        "        # This is required to match the shape with the linear layer.\n",
        "        y = y.view(y.size(0), -1)\n",
        "\n",
        "        y = self.relu4(self.fc(y))\n",
        "        y = self.output(y)\n",
        "\n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJr9T61FtZA4"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg4p0kXBS2Y0"
      },
      "source": [
        "Here we start the actual training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXS0IVlIRhqr"
      },
      "source": [
        "env = create_environment()\n",
        "model = DQN(80, 105, 1, env.action_space.n).to(device)  # Don't forget to update these values after resizing or normalizing the images!\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.00025)\n",
        "memory = SequentialMemory(100000)\n",
        "\n",
        "num_episodes = 1000\n",
        "num_episodes_between_videos = 50\n",
        "\n",
        "# Define an agent that uses the experience buffer and value-function model that we created before.\n",
        "# The agent will try to optimize the value-function to estimate the value of each possible state/action pair.\n",
        "#\n",
        "# The agent uses a decaying epsilon-greedy policy to determine the action to execute in the environment.\n",
        "# We're moving from 100% random to 0.1% chance of random actions in 1M steps. \n",
        "# This means, we'll be totally random at the start and slowly moving towards a deterministic policy.\n",
        "# \n",
        "# Note on the gamma parameter: this controls the discount of future rewards when it comes to estimating\n",
        "# the value-function. A reward received from the current action accounts for 100% of the value of an action\n",
        "# in the current state. Any possible action in the next state only contributes 99% of its reward towards the\n",
        "# value in the current state!\n",
        "# \n",
        "# Note on the tau parameter: this controls the speed at which we update the value-function network in the agent.\n",
        "# We're using a pair of neural networks in the agent to help with the optimization process for the value-function.\n",
        "# Setting a tau value larger than 1 means that the target network only gets updated after x steps. A value of less\n",
        "# then 1 means that we're gradually updating the target network.\n",
        "# The target network construct is important, because it allows us to more predictably learn the value-function.\n",
        "agent = DQNAgent(env.action_space.n, model, loss, \n",
        "    optimizer, memory, tau=10000, gamma=0.99, warmup_steps=50000, update_steps=3,\n",
        "    policy=DecayingEpsilonGreedyPolicy(env.action_space.n, 1.0, 1000000, 0.1, 1.0))\n",
        "\n",
        "episodes = []\n",
        "\n",
        "# We're going to play 100 (= num_episodes) games of space invaders.\n",
        "# This will take a very long time! Using a GPU or TPU is advised.\n",
        "# Note that both are available through Google Colab.\n",
        "\n",
        "# for i in range(num_episodes):\n",
        "for episode in range(num_episodes):\n",
        "    if (episode + 1) % num_episodes_between_videos == 0:\n",
        "        # Resume rendering\n",
        "        env.resume()\n",
        "    else:\n",
        "        # Pause rendering\n",
        "        env.pause()\n",
        "\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "\n",
        "    # Run the episode in terms of timesteps (t)\n",
        "    # This is where the bot actually plays the game\n",
        "    for t in count():\n",
        "        # if (episode + 1) % num_episodes_between_videos == 0:\n",
        "        #     env.render()\n",
        "\n",
        "        # Choose an action and perform it on the environment.\n",
        "        # The output is a new state, the reward and termination condition.\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "\n",
        "        # Record the experience with the agent and train the agent.\n",
        "        # This performs a single backward step through the neural network\n",
        "        # using a number of previous state/action combinations to find\n",
        "        # a better value-estimation function.\n",
        "        agent.record(state, action, next_state, reward, done)\n",
        "        agent.train()\n",
        "\n",
        "        # Store the new state as the current state \n",
        "        # and move on to the next step if we haven't reached the termination point.\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            # print('Episode %i finished in %i timesteps with reward %f' % (i, t, episode_reward))\n",
        "            print('Episode %i finished in %i timesteps with reward %f' % (episode, t, episode_reward))\n",
        "            break\n",
        "\n",
        "env.play()\n",
        "env.close()\n",
        "# show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnTHqvgaD61e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}