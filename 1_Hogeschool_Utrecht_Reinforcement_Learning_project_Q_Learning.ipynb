{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_Hogeschool-Utrecht_Reinforcement-Learning-project_Q-Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basanaz/hu-reinforcement-learning-gym/blob/master/1_Hogeschool_Utrecht_Reinforcement_Learning_project_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eU2QJ--T4dd"
      },
      "source": [
        "# Reinforcement Learning project - Q-Learning\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qdkwbpRUhzO"
      },
      "source": [
        "## Aim\n",
        "In this lab we are going to solve two simple [OpenAI Gym](https://gym.openai.com/) environments using [Q-Learning](https://en.wikipedia.org/wiki/Q-learning). Specifically, the [CartPole-v0](https://gym.openai.com/envs/CartPole-v0/) and [MountainCar-v0](https://gym.openai.com/envs/MountainCar-v0/) environments.\n",
        "\n",
        "We will try to create a table containing the expected reward for each combination of a *state* and *action*. We will use this table to choose the (hopefully) best action given the state the system is in.\n",
        "\n",
        "While this may not be the most advanced or complicated model there is, it is perfect for this task! Furthermore, it can be trained in a relatively short time!\n",
        "\n",
        "## Runtime and environment\n",
        "This [Jupyter Notebook](https://jupyterlab.readthedocs.io/en/latest/) was made to run on Google Colab. For this training, we recommend using the Google Colab environment.\n",
        "\n",
        "Please read the [instructions on Google Colab](https://medium.com/swlh/the-best-place-to-get-started-with-ai-google-colab-tutorial-for-beginners-715e64bb603b) to get started quickly. It behaves similar to Jupyter Notebook, Jupyter Hub and Jupyter Lab, so if you have any experience with those, you're good to go!\n",
        "\n",
        "Some notes on Google Colab:\n",
        "- **Processes in Google Colab won't run forever**. These may be terminated at any time when the platform is crowded, and *will definitely* terminate after 12 hours. To maintain persistency, you can attach the session to **Google Drive** and have your models persist themselves to the Google Drive periodically.\n",
        "- You can enable GPU or TPU support! You can find this option under *Runtime* -> *Change runtime type*.\n",
        "- After installing dependencies, you need to restart the runtime in order to actually use them.\n",
        "\n",
        "If you want to run the code on your own platform or system, you need to keep a few things in mind:\n",
        "- The dependencies you need to install may differ from the ones we installed here. The installed dependencies are suitable for Google Colab, Ubuntu, and Debian.\n",
        "- Since Google Colab isn't attached to a monitor, we render the output to a video file. On your own machine the built-in render method from OpenAI's Gym may suffice.\n",
        "- The default paths use Google Drive! Change these.\n",
        "\n",
        "## Info Support\n",
        "This assignment was developed by Info Support. Looking for a graduation project or job? Check out their website: https://carriere.infosupport.com/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7mfjQyuT_zV"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_LwZ18PXsaL"
      },
      "source": [
        "Some dependencies need to be installed for the code to work. Furthermore, we will define some methods which allow us to show the OpenAI Gym renderings in this (headless) Google Colab environment.\n",
        "\n",
        "You only have to run these and don't need to change any of the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBdwK87YUI9Y"
      },
      "source": [
        "# Install dependencies\n",
        "\"\"\"Note: if you are running this code on your own machine, you probably don't need all of these.\n",
        "   Start with 'pip install gym' and install more packages if you run into errors.\"\"\"\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg cmake > /dev/null 2>&1\n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install colabgymrender"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7IG_wxod9mW"
      },
      "source": [
        "# Imports for helper functions\n",
        "import base64\n",
        "import io\n",
        "import math\n",
        "from pathlib import Path\n",
        "\n",
        "import gym\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from colabgymrender.recorder import Recorder\n",
        "from google.colab import drive\n",
        "from gym.wrappers import Monitor\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFDD54_Afgs4"
      },
      "source": [
        "# Mount your Google Drive. By doing so, you can store any output, models, videos, and images persistently.\n",
        "\n",
        "# Note: in this new version of the notebook, we no longer render video's to Google Drive, but to a temporary storage within the container!\n",
        "# Please copy files from /tmp/HU_RL/ to /content/gdrive/My Drive/<my_path> that you want to keep, or just download them through the viewer!\n",
        "\n",
        "enable_gdrive = False   # Switch to indicate whether Google Drive should be mounted.\n",
        "\n",
        "if enable_gdrive:\n",
        "    drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJeiCQFRgnSj"
      },
      "source": [
        "# Create a directory to store the data for this lab. Feel free to change this.\n",
        "if enable_gdrive:\n",
        "    drive_path = Path('/content/gdrive/My Drive/Colab Notebooks/HU_RL/part1')\n",
        "    drive_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "tmp_path_on_container = Path('/tmp/HU_RL/')\n",
        "video_path = tmp_path_on_container / 'video'\n",
        "video_path.mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU2ZsnBEelqW"
      },
      "source": [
        "# Define helper functions to visually show what the models are doing.\n",
        "%matplotlib inline\n",
        "\n",
        "gym.logger.set_level(gym.logger.ERROR)\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "# def show_video():\n",
        "#     # Display the stored video file\n",
        "#     # Credits: https://star-ai.github.io/Rendering-OpenAi-Gym-in-Colaboratory/\n",
        "#     mp4list = list(data_path.glob('video/*.mp4'))\n",
        "#     if len(mp4list) > 0:\n",
        "#         mp4 = mp4list[-1]\n",
        "#         video = io.open(mp4, 'r+b').read()\n",
        "#         encoded = base64.b64encode(video)\n",
        "#         ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "#                 loop controls style=\"height: 400px;\">\n",
        "#                 <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "#             </video>'''.format(encoded.decode('ascii'))))\n",
        "#     else: \n",
        "#         print('Could not find video')\n",
        "\n",
        "# def record_episode(idx):\n",
        "#     # This determines which episodes to record.\n",
        "#     # Since the video rendering in the OpenAI Gym is a bit buggy, we simply override it and decide\n",
        "#     # whether or not to render inside of our training loop.\n",
        "#     return True\n",
        "\n",
        "def video_env(env):\n",
        "    # Wraps the environment to write its output to a video file\n",
        "    # env = Monitor(env, video_path, video_callable=record_episode, force=True)\n",
        "    env = Recorder(env, video_path, 15)     # Render at 15 FPS. Try to keep this low, to prevent hitting quotas.\n",
        "    return env\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShMPjUFXiwli"
      },
      "source": [
        "# Test the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3hAfAGhi4KK"
      },
      "source": [
        "\"\"\"We will use a basic OpenAI Gym examle: CartPole-v0.\n",
        "In this example, we will try to balance a pole on a cart.\n",
        "This is similar to kids (and.. grown-ups) trying to balance sticks on their hands.\n",
        "\n",
        "Check out the OpenAI Gym documentation to learn more: https://gym.openai.com/docs/\"\"\"\n",
        "\n",
        "# Create the desired environment\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# Wrap the environment, to make sure we get to see a fancy video\n",
        "env = video_env(env)\n",
        "\n",
        "# Before you can use a Gym environment, it needs to be reset.\n",
        "state = env.reset()\n",
        "\n",
        "# Perform random actions untill we drop the stick. Just as an example.\n",
        "done = False\n",
        "while not done:\n",
        "    # env.render()  # This line is no longer needed\n",
        "    # The action_space contains all possible actions we can take.\n",
        "    random_action = env.action_space.sample() \n",
        "\n",
        "    # After each action, we end up in a new state and receive a reward.\n",
        "    # When we drop the pole (more than 12 degrees), or balance it long enough (200 steps),\n",
        "    # or drive off the screen, done is set to True.\n",
        "    state, reward, done, info = env.step(random_action)\n",
        "\n",
        "# Show the results!\n",
        "env.play()  # Render the video\n",
        "# env.close()   # We keep the env open this time\n",
        "# show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-O3zaP6q4-E"
      },
      "source": [
        "# Neat, it did something (randomly)! \n",
        "\n",
        "# In order to train the system, we will try to predict the reward a certain actions yields given the state of the system.\n",
        "# But what is the state anyway?\n",
        "\n",
        "# In this environment, the state represents the cart's position and velocity, and the pole's angle and velocity.\n",
        "\n",
        "# Let's check out the current state\n",
        "print(f'Cart position: {state[0]} (range: [-4.8, 4.8])')\n",
        "print(f'Cart velocity: {state[1]} (range: [-inf, inf])')\n",
        "print(f'Pole angle: {state[2]} (range: [-0.418, 0.418])')\n",
        "print(f'Pole velocity: {state[3]} (range [-inf, inf])')\n",
        "\n",
        "# You can find out the minimum and maximum possible observation values using:\n",
        "print(f'Low observation space:', env.observation_space.low)\n",
        "print(f'High observation space:', env.observation_space.low)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhQlBt2hUAQ2"
      },
      "source": [
        "# Implement Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFsyJOo5GrZe"
      },
      "source": [
        "## Task\n",
        "Implement Q-Learning and find suitable parameters to reach a 200 reward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDvyHdRyRtdq"
      },
      "source": [
        "# Define parameters - Fill in the dots\n",
        "\n",
        "num_episodes = ...\n",
        "num_episodes_between_status_update = 500\n",
        "num_episodes_between_videos = 5000\n",
        "\n",
        "learning_rate = ...         # also known as: alpha\n",
        "discount = ...              # also known as: gamma\n",
        "epsilon = ...\n",
        "\n",
        "# Epsilon decay\n",
        "pass    # Optionally, add parameters for epsilon decay here\n",
        "\n",
        "# Discretization\n",
        "pass    # You can add parameters to discretize states here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0S8rYCWT15s"
      },
      "source": [
        "## Q-Table creation\n",
        "\n",
        "# As seen before, the state consists of 4 floating point values.\n",
        "# It makes sense to discretize these values (read: place them in buckets), to reduce the state space and therefore the Q-table size\n",
        "state_shape = [..., ..., ..., ...]      # For instance: [4, 4, 6, 6], or [10] * 4, or [200, 200, 100, 100]\n",
        "\n",
        "# Define the initial Q table as a random uniform distribution\n",
        "q_table = np.random.uniform(low=-2, high=0, size=(state_shape + [env.action_space.n]))\n",
        "\n",
        "#print('Initial Q table:', q_table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4Cp8cNtUcDf"
      },
      "source": [
        "# Train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SMMA_KceHrx"
      },
      "source": [
        "# Functions\n",
        "\n",
        "def discretize_state(state):\n",
        "    # A Q-table cannot practically handle infinite states, so limit the state space by\n",
        "    # discretizing the state into buckets.\n",
        "    discrete_state = ...\n",
        "    return tuple(discrete_state.astype(np.int))\n",
        "\n",
        "def take_action(discrete_state, epsilon):\n",
        "    # Take an action to either explore or exploit.\n",
        "    action = ...\n",
        "    return action\n",
        "\n",
        "def estimated_max_for_next_state(discrete_state):\n",
        "    # What's the best expected Q-value for the next state?\n",
        "    estimated_max = ...\n",
        "    return estimated_max\n",
        "\n",
        "def new_q_value(discrete_state, action, max_future_q):\n",
        "    # Calculate the new Q-value\n",
        "    current_q = ...\n",
        "    new_q = ...\n",
        "    return new_q\n",
        "\n",
        "def decayed_epsilon(epsilon, episode):\n",
        "    # Optionally, decay the epsilon value\n",
        "    pass\n",
        "    return epsilon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMUc3G164cA7"
      },
      "source": [
        "# Time to train the system\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    if (episode + 1) % num_episodes_between_videos == 0:\n",
        "        # Resume rendering\n",
        "        env.resume()\n",
        "    else:\n",
        "        # Pause rendering\n",
        "        env.pause()\n",
        "\n",
        "    state = env.reset() # Don't forget to reset the environment between episodes\n",
        "    current_state_disc = discretize_state(state)\n",
        "\n",
        "    reward_sum = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        # if (episode + 1) % num_episodes_between_videos == 0:\n",
        "        #     env.render()\n",
        "\n",
        "        # Take an action by exploration or exploitation\n",
        "        action = take_action(current_state_disc, epsilon)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        new_state_disc = discretize_state(new_state)\n",
        "\n",
        "        # Calculate the total reward\n",
        "        reward_sum += reward\n",
        "\n",
        "        if not done:\n",
        "            # Retrieve the maximum estimated value for the next state\n",
        "            max_future_q = estimated_max_for_next_state(new_state_disc)\n",
        "\n",
        "            # Calculate the new value (note: Bellman equation)\n",
        "            new_q = new_q_value(current_state_disc, action, max_future_q)\n",
        "            q_table[current_state_disc + (action,)] = new_q\n",
        "        else:\n",
        "            #     # Render the video\n",
        "            #     if (episode + 1) % num_episodes_between_status_update == 0:\n",
        "            #         env.render()\n",
        "            #         print(f'Total reward at episode {episode + 1}: {reward_sum}')\n",
        "            pass\n",
        "\n",
        "        # Prepare for the next loop\n",
        "        current_state_disc = new_state_disc\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = decayed_epsilon(epsilon, episode)\n",
        "\n",
        "    if (episode + 1) % num_episodes_between_status_update == 0:\n",
        "        print(f'Finished episode {episode + 1} / {num_episodes} with reward {reward_sum}')\n",
        "\n",
        "env.play()\n",
        "env.close()\n",
        "# show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RbC9GKag15q"
      },
      "source": [
        "# MountainCar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0oyL2bxg_1j"
      },
      "source": [
        "Now apply the things you've learned to the MountainCar problem. Please note that the observable space differs from the previous problem. Thus, before you start training, you need to learn more about thethis new environment.\n",
        "\n",
        "Here is some code to help you get started.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWzVnM10g4Oc"
      },
      "source": [
        "# Create the desired environment\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "# Wrap the environment, to make sure we get to see a fancy video\n",
        "env = video_env(env)\n",
        "\n",
        "# Before you can use a Gym environment, it needs to be reset.\n",
        "state = env.reset()\n",
        "\n",
        "# Perform random actions untill we drop the stick. Just as an example.\n",
        "done = False\n",
        "while not done:\n",
        "    # Explore and take actions\n",
        "    pass\n",
        "\n",
        "    # Remove the line below when you have created an implementation you want to test.\n",
        "    done = True\n",
        "\n",
        "# Show the results!\n",
        "env.play()\n",
        "env.close()\n",
        "# show_video()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}